import logging
import requests, time, os, subprocess, atexit
from pathlib import Path
import pandas as pd
from typing import Dict, List
from langchain_core.documents import Document
from pipelines.generation import format_document_for_context

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def save_results(
    df: pd.DataFrame, 
    predictions: List[str], 
    citations: List[str], 
    all_retrieved_docs: Dict[str, List[Document]], 
    elapsed_times: List[float],  # <--- [ìˆ˜ì •] elapsed_times íŒŒë¼ë¯¸í„° ì¶”ê°€
    out_dir: Path, 
    version: str
):
    """
    - ì›ë³¸ CSVì˜ ëª¨ë“  ì—´ì„ ìœ ì§€í•©ë‹ˆë‹¤.
    - ìƒˆë¡œ ê²€ìƒ‰ëœ ë¬¸ì„œëŠ” 'prediction_retrieved_article_name_*' ì—´ì— ì €ì¥í•©ë‹ˆë‹¤.
    - Prediction, Citation, elapsed_times ì—´ì„ ìƒˆë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    out_dir.mkdir(parents=True, exist_ok=True)
    output_path_csv = out_dir / f"final_submit_{version}.csv"

    # ì›ë³¸ DataFrameì˜ ë³µì‚¬ë³¸ì„ ë§Œë“¤ì–´ ì‘ì—… ìˆ˜í–‰
    output_df = df.copy()

    # ìƒˆë¡œ ìƒì„±ëœ ê²°ê³¼ë“¤ì„ ìƒˆ ì—´ë¡œ ì¶”ê°€
    output_df['Prediction'] = predictions
    output_df['Citation'] = citations
    output_df['elapsed_times'] = elapsed_times

    # ì´ë²ˆ ì‹¤í–‰ì—ì„œ ìƒˆë¡œ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ 'prediction_retrieved_article_name_*' ì—´ì— ì €ì¥
    max_docs = 0
    if all_retrieved_docs:
        max_docs = max((len(docs) for docs in all_retrieved_docs.values()), default=0)

    # ìµœëŒ€ 50ê°œ ë¬¸ì„œê¹Œì§€ ì €ì¥
    num_docs_to_save = min(max_docs, 50)
    for i in range(num_docs_to_save):
        col_name = f"prediction_retrieved_article_name_{i+1}"
        output_df[col_name] = output_df['id'].astype(str).map(
            {sid: format_document_for_context(docs[i]) if i < len(docs) else "" for sid, docs in all_retrieved_docs.items()}
        )
    
    # ë§Œì•½ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ 50ê°œ ë¯¸ë§Œì¼ ê²½ìš°, ë‚˜ë¨¸ì§€ ì—´ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›Œ ìŠ¤í‚¤ë§ˆë¥¼ í†µì¼í•©ë‹ˆë‹¤.
    for i in range(num_docs_to_save, 50):
         col_name = f"prediction_retrieved_article_name_{i+1}"
         output_df[col_name] = ""

    # --- CSV íŒŒì¼ ì €ì¥ ---
    # ì´ì œ output_dfëŠ” ì›ë³¸ ì—´ + ì‹ ê·œ ì—´ì„ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤.
    try:
        output_df.to_csv(output_path_csv, index=False, encoding='utf-8-sig')
        logging.info(f"CSV ê²°ê³¼ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: {output_path_csv}")
    except Exception as e:
        logging.error(f"ì˜¤ë¥˜: CSV íŒŒì¼ ì €ì¥ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ - {e}")

def set_global_device(device_id: int):
    """ëª¨ë“  í•˜ìœ„ í”„ë¡œì„¸ìŠ¤ì™€ torch/pipelineì´ ì§€ì •ëœ GPUë§Œ ë³´ë„ë¡ í™˜ê²½ ê³ ì •"""
    os.environ["CUDA_VISIBLE_DEVICES"] = str(device_id)
    logging.info(f"âœ… CUDA_VISIBLE_DEVICES={device_id} ì„¤ì • ì™„ë£Œ (ëª¨ë“  ëª¨ë“ˆì€ ì´ GPUë§Œ ì‚¬ìš©)")

def wait_for_vllm(port: int, timeout: int = 600):
    url = f"http://localhost:{port}/v1/models"
    t0 = time.time()
    while True:
        try:
            r = requests.get(url, timeout=5)
            if r.status_code == 200:
                logging.info(f"âœ… vLLM ì„œë²„ ì¤€ë¹„ ì™„ë£Œ: {url}")
                break
        except Exception:
            pass
        if time.time() - t0 > timeout:
            raise TimeoutError(f"vLLM ì„œë²„ê°€ {timeout}ì´ˆ ì•ˆì— ì¤€ë¹„ë˜ì§€ ì•ŠìŒ.")
        time.sleep(5)

def launch_vllm_server(model: str, port: int, mem_util: float = 0.8):
    """vllm ì„œë²„ë¥¼ ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰í•˜ê³  ì¢…ë£Œ ì‹œ ìë™ìœ¼ë¡œ kill"""
    cmd = [
        "python", "-m", "vllm.entrypoints.openai.api_server",
        "--model", model,
        "--port", str(port),
        "--gpu-memory-utilization", str(mem_util),
    ]
    proc = subprocess.Popen(cmd, env=os.environ)  
    atexit.register(proc.terminate)
    logging.info(f"ğŸš€ vLLM ì„œë²„ ì‹¤í–‰: {model} (port={port}, CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']})")
    return proc